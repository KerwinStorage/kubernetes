å‰è¨€ï¼šä¸€å¼€å§‹å‡†å¤‡éƒ¨ç½² [2.0](https://www.elastic.co/guide/en/cloud-on-k8s/2.0/k8s-deploy-eck.html) ç‰ˆæœ¬åŠä»¥ä¸Šç‰ˆæœ¬ecké›†ç¾¤ï¼Œä½†æ˜¯åœ¨éƒ¨ç½² [elasticsearch](https://www.elastic.co/guide/en/cloud-on-k8s/2.0/k8s-deploy-elasticsearch.html) æ—¶æ­»æ´»æŒ‚è½½ä¸ä¸Špvcï¼Œ`kubectl get pv,pvc`å‘ç°pvå’Œpvcå·²ç»æ˜¯`Bound`çŠ¶æ€ï¼Œä½†æ˜¯podå†…ä¾ç„¶æœ‰`0/3 nodes are available: 3 pod has unbound immediate PersistentVolumeClaims.`æŒ‚è½½ä¸ä¸Šé—®é¢˜ğŸ˜’ï¼Œé€€è€Œæ±‚æ­¤æ¬¡éƒ¨ç½²äº†[1.1 ç‰ˆæœ¬eck](https://www.elastic.co/guide/en/cloud-on-k8s/1.1/k8s_supported_versions.html) ã€‚ğŸ˜‚

å‚è€ƒæ–‡æ¡£ï¼šhttps://www.codenong.com/cs105948927/

- https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-quickstart.html
- https://github.com/elastic/cloud-on-k8s/tree/master/config/recipes/beats
- https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-volume-claim-templates.html
- https://github.com/elastic/cloud-on-k8s/tree/master/config/samples

å®˜ç½‘ï¼šhttps://www.elastic.co/cn/elastic-cloud-kubernetesï¼Œé¡¹ç›®åœ°å€ï¼šhttps://github.com/elastic/cloud-on-k8s

## 1. StorageClassåˆ›å»º

| hostname     | ip address    | service   |
| ------------ | ------------- | --------- |
| k8s-master01 | 192.168.80.45 | nfsæœåŠ¡ç«¯ |
| k8s-node01   | 192.168.80.46 | nfså®¢æˆ·ç«¯ |
| k8s-node02   | 192.168.80.47 | nfså®¢æˆ·ç«¯ |

### 1.1 nfsæœåŠ¡å®‰è£…

- k8s-master01æ“ä½œ

```bash
apt install nfs-kernel-server nfs-common
mkdir /nfs # æŒ‚åœ¨ç›®å½•
sudo vim /etc/exports
/nfs *(rw,sync,no_root_squash,no_subtree_check)

#é‡å¯
exportfs -a
systemctl restart nfs-kernel-server
systemctl enable nfs-kernel-server
```

- k8s-node01ã€k8s-node02æŒ‚è½½

```bash
apt install nfs-common
mkdir -p /nfs/
mount -t nfs 192.168.80.45:/nfs/ /nfs/
df -h

# è®¾ç½®å¯åŠ¨è‡ªåŠ¨æŒ‚è½½
vim /etc/fstab
192.168.80.45:/nfs /nfs         nfs     defaults                 0       0
```

### 1.2 åˆ›å»ºå­˜å‚¨

```bash
cat > nfs-storage.yaml << EOF
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-storage
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: k8s-sigs.io/nfs-subdir-external-provisioner
parameters:
  archiveOnDelete: "true"  ## åˆ é™¤pvçš„æ—¶å€™ï¼Œpvçš„å†…å®¹æ˜¯å¦è¦å¤‡ä»½


---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-client-provisioner
  labels:
    app: nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: nfs-client-provisioner
  template:
    metadata:
      labels:
        app: nfs-client-provisioner
    spec:
      serviceAccountName: nfs-client-provisioner
      containers:
        - name: nfs-client-provisioner
          image: registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/nfs-subdir-external-provisioner:v4.0.2
          # resources:
          #    limits:
          #      cpu: 10m
          #    requests:
          #      cpu: 10m
          volumeMounts:
            - name: nfs-client-root
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME
              value: k8s-sigs.io/nfs-subdir-external-provisioner
            - name: NFS_SERVER
              value: 192.168.80.45 ## æŒ‡å®šè‡ªå·±nfsæœåŠ¡å™¨åœ°å€
            - name: NFS_PATH
              value: /nfs/  ## nfsæœåŠ¡å™¨å…±äº«çš„ç›®å½•
      volumes:
        - name: nfs-client-root
          nfs:
            server: 192.168.80.45 #æ³¨æ„è¿™é‡Œçš„åœ°å€
            path: /nfs/
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-client-provisioner-runner
rules:
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch", "create", "delete"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create", "update", "patch"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: run-nfs-client-provisioner
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    # replace with namespace where provisioner is deployed
    namespace: default
roleRef:
  kind: ClusterRole
  name: nfs-client-provisioner-runner
  apiGroup: rbac.authorization.k8s.io
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
rules:
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["get", "list", "watch", "create", "update", "patch"]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    # replace with namespace where provisioner is deployed
    namespace: default
roleRef:
  kind: Role
  name: leader-locking-nfs-client-provisioner
  apiGroup: rbac.authorization.k8s.io
EOF

# åˆ›å»ºé»˜è®¤å­˜å‚¨ 
kubectl apply -f nfs-storage.yaml

# æŸ¥çœ‹åˆ›å»ºæƒ…å†µ
kubectl get storageclasses.storage.k8s.io
NAME                    PROVISIONER                                   RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
nfs-storage (default)   k8s-sigs.io/nfs-subdir-external-provisioner   Delete          Immediate           false                  33m
```

## 2. éƒ¨ç½²eck

```bash
kubectl get nodes
NAME          STATUS   ROLES    AGE   VERSION
k8s-master1   Ready    master   21d   v1.19.11
k8s-node01    Ready    node     21d   v1.19.11
k8s-node02    Ready    node     21d   v1.19.11
```

æ³¨ï¼šè¿™é‡ŒmasterèŠ‚ç‚¹å…è®¸éƒ¨ç½²podã€‚

### 2.1 å®‰è£…eck operator

```bash
kubectl apply -f https://download.elastic.co/downloads/eck/1.1.0/all-in-one.yaml
# æŸ¥çœ‹podå¯åŠ¨
kubectl -n elastic-system get pods
NAME                 READY   STATUS    RESTARTS   AGE
elastic-operator-0   1/1     Running   1          81s
```

æŸ¥çœ‹åˆ›å»ºçš„crdï¼Œåˆ›å»ºäº†3ä¸ªcrdï¼Œapmserverã€elasticsearcheä»¥åŠkibana.

```bash
kubectl get crd | grep elastic
apmservers.apm.k8s.elastic.co                         2022-11-16T13:27:59Z
elasticsearches.elasticsearch.k8s.elastic.co          2022-11-16T13:27:59Z
kibanas.kibana.k8s.elastic.co                         2022-11-16T13:27:59Z
```

### 2.2 éƒ¨ç½²eså’Œkibana

å…‹éš†githubæºç ä¸­çš„ç¤ºä¾‹yamlåˆ°æœ¬åœ°

```bash
curl -LO https://github.com/elastic/cloud-on-k8s/archive/1.1.0.tar.gz
tar zxvf 1.1.0.tar.gz
cd cloud-on-k8s-1.1.0/config/recipes/beats/
ll
0_ns.yaml  1_monitor.yaml  2_filebeat-kubernetes.yaml  3_metricbeat-kubernetes.yaml
```

åˆ›å»ºå‘½åç©ºé—´ï¼š

```bash
kubectl apply -f 0_ns.yaml
```

éƒ¨ç½²eså’Œkibanaï¼Œé…ç½® storageClassNameä¸ºnfs-clientï¼ŒæœåŠ¡ç±»å‹æ”¹ä¸ºnodePortã€‚ï¼š

```bash
cat > 1_monitor.yaml <<EOF
apiVersion: elasticsearch.k8s.elastic.co/v1
kind: Elasticsearch
metadata:
  name: monitor
  namespace: beats
spec:
  version: 7.6.2
  nodeSets:
  - name: mdi
    count: 3
    config:
      node.master: true
      node.data: true
      node.ingest: true
      node.store.allow_mmap: false
    volumeClaimTemplates:
    - metadata:
        name: elasticsearch-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 50Gi
        storageClassName: nfs-storage
  http:
    service:
      spec:
        type: NodePort
---
apiVersion: kibana.k8s.elastic.co/v1
kind: Kibana
metadata:
  name: monitor
  namespace: beats
spec:
  version: 7.6.2
  count: 1
  elasticsearchRef:
    name: "monitor"
  http:
    service:
      spec:
        type: NodePort
EOF
```

æ‰§è¡Œyamlæ–‡ä»¶éƒ¨ç½²eså’Œkibanaï¼š

```bash
kubectl apply -f 1_monitor.yaml

kubectl get pod -n beats
NAME                          READY   STATUS    RESTARTS   AGE
monitor-es-mdi-0              1/1     Running   0          2m44s
monitor-es-mdi-1              1/1     Running   0          2m43s
monitor-es-mdi-2              1/1     Running   0          2m43s
monitor-kb-7df4ffb4d9-b2qbf   1/1     Running   0          2m44s
```

æŸ¥çœ‹åˆ›å»ºçš„Elasticsearchå’Œkibanaèµ„æºï¼ŒåŒ…æ‹¬è¿è¡ŒçŠ¶å†µï¼Œç‰ˆæœ¬å’ŒèŠ‚ç‚¹æ•°ï¼š

```bash
kubectl get elasticsearch -n beats
NAME      HEALTH   NODES   VERSION   PHASE   AGE
monitor   green    3       7.6.2     Ready   3m29s

kubectl get kibana -n beats
NAME      HEALTH   NODES   VERSION   AGE
monitor   green    1       7.6.2     4m
```

æŸ¥çœ‹åˆ›å»ºçš„pvå’Œpvcï¼š

```bash
kubectl -n beats get pvc
NAME                                                        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/elasticsearch-data-monitor-es-mdi-0   Bound    pvc-831c8864-e0c3-4a02-8ce8-3bacc5d0738f   50Gi       RWO            nfs-storage    4m51s
persistentvolumeclaim/elasticsearch-data-monitor-es-mdi-1   Bound    pvc-b65463af-0553-4b73-8d61-a7d8d7f5a6c2   50Gi       RWO            nfs-storage    4m50s
persistentvolumeclaim/elasticsearch-data-monitor-es-mdi-2   Bound    pvc-4e50ab12-737a-4b6e-ab72-5ca499cb2ac8   50Gi       RWO            nfs-storage    4m50s

kubectl -n beats get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                                       STORAGECLASS   REASON   AGE
pvc-4e50ab12-737a-4b6e-ab72-5ca499cb2ac8   50Gi       RWO            Delete           Bound    beats/elasticsearch-data-monitor-es-mdi-2   nfs-storage             5m26s
pvc-69d5e974-61eb-4868-9d34-6d429cc3ca6b   20Gi       RWX            Delete           Bound    default/eck-pvc                             nfs-storage             3d11h
pvc-831c8864-e0c3-4a02-8ce8-3bacc5d0738f   50Gi       RWO            Delete           Bound    beats/elasticsearch-data-monitor-es-mdi-0   nfs-storage             5m26s
pvc-b65463af-0553-4b73-8d61-a7d8d7f5a6c2   50Gi       RWO            Delete           Bound    beats/elasticsearch-data-monitor-es-mdi-1   nfs-storage             5m26s
```

æŸ¥çœ‹åˆ›å»ºçš„serviceï¼Œéƒ¨ç½²æ—¶å·²ç»å°†eså’ŒkibanaæœåŠ¡ç±»å‹æ”¹ä¸ºNodePortï¼Œæ–¹ä¾¿ä»é›†ç¾¤å¤–è®¿é—®ã€‚ï¼š

```bash
kubectl -n beats get svc
NAME                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
monitor-es-http        NodePort    10.254.229.106   <none>        9200:30938/TCP   6m51s
monitor-es-mdi         ClusterIP   None             <none>        <none>           6m46s
monitor-es-transport   ClusterIP   None             <none>        9300/TCP         6m51s
monitor-kb-http        NodePort    10.254.128.69    <none>        5601:32133/TCP   6m48s
```

é»˜è®¤elasticsearchå¯ç”¨äº†éªŒè¯ï¼Œè·å–elasticç”¨æˆ·çš„å¯†ç ï¼š

```bash
PASSWORD=$(kubectl -n beats get secret monitor-es-elastic-user -o=jsonpath='{.data.elastic}' | base64 --decode)
echo $PASSWORD
```

æµè§ˆå™¨è®¿é—®elasticsearchï¼š

[https://192.168.80.45:30938](https://192.168.80.45:30938/)ï¼ˆä¼šæœ‰ä¸€ä¸ªå¼¹çª—å‡ºæ¥ï¼‰

userï¼šelasticï¼Œpasswordï¼šï¼ˆä¸Šé¢çš„`echo $PASSWORD`ï¼‰

```bash
{
  "name" : "monitor-es-mdi-2",
  "cluster_name" : "monitor",
  "cluster_uuid" : "KHl5tzDSSHa1Lnt4qKmy5g",
  "version" : {
    "number" : "7.6.2",
    "build_flavor" : "default",
    "build_type" : "docker",
    "build_hash" : "ef48eb35cf30adf4db14086e8aabd07ef6fb113f",
    "build_date" : "2020-03-26T06:34:37.794943Z",
    "build_snapshot" : false,
    "lucene_version" : "8.4.0",
    "minimum_wire_compatibility_version" : "6.8.0",
    "minimum_index_compatibility_version" : "6.0.0-beta1"
  },
  "tagline" : "You Know, for Search"
}
```

åœ¨æµè§ˆå™¨ä¸­è®¿é—®kibanaï¼Œç”¨æˆ·å¯†ç ä¸elasticsearchç›¸åŒï¼Œé€‰æ‹©Explore on my ownï¼Œå¯ä»¥çœ‹åˆ°è¿˜æ²¡æœ‰åˆ›å»ºindexã€‚

[https://192.168.80.45:32133](https://192.168.80.45:32133/)ï¼Œuserï¼šelasticï¼Œpasswordï¼šï¼ˆä¸Šé¢çš„`echo $PASSWORD`ï¼‰

![img](https://img2022.cnblogs.com/blog/1740081/202211/1740081-20221116231914824-1619658547.png)

## 3. éƒ¨ç½²filebeat

```bash
sed -i 's#docker.elastic.co/beats/filebeat:7.6.0#elastic/filebeat:7.6.2#g' 2_filebeat-kubernetes.yaml
kubectl apply -f 2_filebeat-kubernetes.yaml
```

æŸ¥çœ‹åˆ›å»ºçš„podsï¼š

```bash
kubectl -n beats get pods -l k8s-app=filebeat
NAME             READY   STATUS    RESTARTS   AGE
filebeat-9vg75   1/1     Running   0          14s
filebeat-hn9tb   1/1     Running   0          14s
filebeat-qd284   1/1     Running   0          14s
```

è®¿é—®kibanaï¼Œæ­¤æ—¶å¯ä»¥æœç´¢åˆ°filebeatçš„indexï¼Œå¡«å†™index patternï¼Œé€‰æ‹©@timestrapç„¶ååˆ›å»ºindex.ï¼š

![img](https://img2022.cnblogs.com/blog/1740081/202211/1740081-20221116232152446-1331498774.png)

![img](https://img2022.cnblogs.com/blog/1740081/202211/1740081-20221116232452945-867102842.png)

ç‚¹å‡»`Create index pattern`ã€‚

![img](https://img2022.cnblogs.com/blog/1740081/202211/1740081-20221116232525502-266297784.png)

## 4. éƒ¨ç½²metricbeat

```bash
sed -i 's#docker.elastic.co/beats/metricbeat:7.6.0#elastic/metricbeat:7.6.2#g' 3_metricbeat-kubernetes.yaml
kubectl apply -f 3_metricbeat-kubernetes.yaml

# æŸ¥çœ‹pod
kubectl -n beats get pods -l k8s-app=metricbeat
NAME                         READY   STATUS    RESTARTS   AGE
metricbeat-95959544b-n5hz9   1/1     Running   0          25s
metricbeat-hlg9b             1/1     Running   0          25s
metricbeat-tttr5             1/1     Running   0          25s
metricbeat-x4z8g             1/1     Running   0          25s
```

## é™„è®®ï¼šå¤„ç†é‡åˆ°çš„é—®é¢˜

é—®é¢˜æè¿°ï¼š`0/3 nodes are available: 3 pod has unbound immediate PersistentVolumeClaims.`

1. pvcæ²¡æœ‰åˆ›å»ºæˆåŠŸï¼ŒåŠ¨æ€pvcç”± `provisioner` æ¥è´Ÿè´£åˆ›å»ºï¼Œéœ€è¦æŸ¥çœ‹æ—¥å¿—`kubectl describe pod nfs-client-provisioner-5c7fbb465d-m69pd`
2. pvcå’Œpvç»‘å®šå®Œæˆï¼Œæœ‰åŒæ ·æŠ¥é”™ï¼Œå¯èƒ½æ˜¯æœ¬åœ°æœºå™¨è¢«é‡å¯è¿‡ç›®å½•æ²¡æœ‰æŒ‚è½½ï¼Œå¯ä»¥å°†æŒ‚è½½å‘½ä»¤å†™å…¥åˆ°æ–‡ä»¶å†…ã€‚